Random Forest

An ensemble learning method for classfication and regression operate by constructing a multiple of decision tree.  
Why use:
  Reasonable fast but very easy to use
  Handles spare data/missing data well  
  Overcomes problem with over fitting
How:
  Tree bagging: random sample with replacement
  Assembly many weak decision tree.  
  Random subset fo the features
  Voting
  
  
  library(randomForest)
  data(iris)
  s <- sample(150,100)
  iris_train = iris[s,]
iris_test = iris[-s,]  
rfm <- randomForest(Species~., iris_train)
summary(rfm)
rfm
Call:
  randomForest(formula = Species ~ ., data = iris_train) 
Type of random forest: classification
Number of trees: 500
No. of variables tried at each split: 2

OOB estimate of  error rate: 3%
Confusion matrix:
  setosa versicolor virginica class.error
setosa         32          0         0  0.00000000
versicolor      0         35         1  0.02777778
virginica       0          2        30  0.06250000

p<-predict(rfm, iris_test)
table(iris_test[,5],p)
p
setosa versicolor virginica
setosa         18          0         0
versicolor      0         12         2
virginica       0          0        18

mean(p == iris_test[,5])
[1] 0.96

importance(rfm) // Which factor is more important.
MeanDecreaseGini
Sepal.Length         6.742286
Sepal.Width          1.952628
Petal.Length        29.097117
Petal.Width         28.017992

